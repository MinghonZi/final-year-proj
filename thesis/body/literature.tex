\section{Literature Survey} \label{sec:literature}

This section aims to provide an overview of some key publications and developments in the field of DRL, as well as review works similar to this project.

\cite{ref:rl-intro} is an introductory-level textbook first published in 1998 that provides a comprehensive introduction to reinforcement learning, including foundational concepts, algorithms, and techniques. \cite{ref:neuro-dp} treats the topic more formally, including rigorous proofs. \cite{ref:spinning-up} is an informal online educational resource covering basic concepts, key literature, and various baseline algorithms.

The emergence of Q-learning \cite{ref:q-learning} represented a significant advancement in the field of Reinforcement Learning during its early stages. Later, \cite{ref:fn-approx} introduced Vanilla Policy Gradient (VPG), which forms the foundation for many later DRL algorithms. The paper demonstrates how policy gradients can be used with function approximation to learn policies in high-dimensional spaces.

\cite{ref:dqn} introduced the concept of using deep neural networks to approximate functions for reinforcement learning. This paper proposed the first DRL algorithm called Deep Q-Network (DQN), which approximates a state-value function from Q-Learning with a neural network, and demonstrated its effectiveness by training an agent to play Atari games. Two years later, \cite{ref:dqn-humanlevel} extended the approach in the previous paper to achieve human-level performance on a suite of Atari games. However, the major shortcoming of this approach is that it relies on the Q-learning framework, which can only be applied to reinforcement learning problems in discrete spaces.

\cite{ref:dpg} introduced Deterministic Policy Gradient (DPG), a policy gradient algorithm with a deterministic actor-critic architecture in which the critic estimates an action-value function while the actor ascends the gradient of the action-value function. \cite{ref:ddpg} proposed the Deep Deterministic Policy Gradient (DDPG) algorithm, which combines the actor-critic architecture with deep neural networks to approximate both the policy and the value function. This extends DQN to continuous action spaces. DDPG is shown to be capable of learning control policies for a range of high-dimensional continuous control tasks. Later, \cite{ref:td3} introduced twin critics technique and policy update delay technique to mitigate value function approximation error, ensuring the policy update at each step is not too large and does not cause unstable performance. The algorithm formed by applying these two techniques to DDPG is called Twin-Delayed DDPG (TD3).

\cite{ref:trpo} introduced a policy gradient algorithm named Trust Region Policy Optimisation (TRPO) that uses trust regions to ensure that policy changes are not excessively significant. \cite{ref:ppo} simplifies TRPO by using a clipped surrogate objective while retaining similar performance. The simplified algorithm is known as Proximal Policy Optimisation (PPO).

\cite{ref:svg} proposed Stochastic Value Gradients (SVG), a generalisation of DPG, which introduces the idea of stochastic policies. The contribution of this work represents a stepping stone between DPG and maximum entropy methods. Based on SVG, \cite{ref:sac} proposed Soft Actor-Critic (SAC) that balances a trade-off between maximising the expected return and maximising the entropy of the policy. In the same year, the original SAC adopted the twin critics technique from TD3 and improved the trade-off strategy by automatically adjusting the ratio of expected return and policy entropy during training. This work is presented in \cite{ref:sac-auto}.

AlphaGo \cite{ref:alphago} combined two deep neural networks with Monte Carlo tree search (MCTS), using both supervised learning and reinforcement learning to train the networks, to master the game of Go. Its successor, AlphaGo Zero \cite{ref:alphago-zero}, improved upon AlphaGo by relying solely on self-play reinforcement learning, excluding the need for human knowledge except for the rules of the game. AlphaZero \cite{ref:alphazero} generalised the AlphaGo Zero approach to play other games like chess and shogi. MuZero \cite{ref:muzero} extended the AlphaZero approach by eliminating the need for knowing the game rules.

The first application of DRL to the mapless navigation task is described in \cite{ref:virtual2real-drl}. Since then, numerous studies have used this application as an experimental case. For example, \cite{ref:energy-efficient} tested the energy efficiency of deep and spiking neural networks with neuromorphic hardware on the mapless navigation task. \cite{ref:huauv} replaced the original differential wheeled robot with a Hybrid Unmanned Aerial Underwater Vehicle (HUAUV), a type of robot that can operate in both air and water media.

Some articles reveal the current challenges and difficulties of DRL. \cite{ref:drl-that-matters} discussed reproducibility issues and provided guidelines for more robust experimentation in DRL. \cite{ref:drl-doesnt-work-yet} criticised DRL in many aspects, including the lack of problem-specific knowledge exploitation, difficult reward shaping, local optima, poor generalisation, and hard-to-reproduce results. \cite{ref:reproducing-drl} specifically discussed the difficulties of reproducing the results of DRL papers. \cite{ref:sim2real} surveyed the sim-to-real transfer gap of DRL for robotics.
